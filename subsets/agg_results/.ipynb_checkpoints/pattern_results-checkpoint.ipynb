{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import rasterio\n",
    "from skimage.measure import profile_line\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_of_lowest_similarity(results):\n",
    "    \n",
    "    df = pd.read_csv(results)\n",
    "    del df['Unnamed: 0']\n",
    "\n",
    "    worst_ssim = df.loc[df.ssim.idxmin()]\n",
    "    worst_nrmse = df.loc[df.nrmse.idxmin()]\n",
    "    worst_cwssim = df.loc[df.cwssim.idxmin()]\n",
    "    worst_gms = df.loc[df.gms.idxmin()]\n",
    "    \n",
    "    metrics = [worst_nrmse, worst_ssim, worst_cwssim, worst_gms]\n",
    "    metric_names = ['nrmse', '_ssim', 'cwssim', 'gms']\n",
    "    \n",
    "    metric_dicts = []\n",
    "    \n",
    "    for m, n in zip(metrics, metric_names):\n",
    "        \n",
    "        d = dict()\n",
    "        d['metric'] = n\n",
    "    \n",
    "        base_path = os.path.join('../', m.Zone, 'raster')\n",
    "    \n",
    "        yr1, yr2 = m.Winters[0:4], m.Winters[-4:]\n",
    "    \n",
    "        iqa_map_path = os.path.join(base_path, 'iqa', m.Winters.replace(' ', '_'))\n",
    "        \n",
    "        d['dem_path'] = os.path.join(base_path, 'dem', m.Zone + '_dem.tif')\n",
    "        \n",
    "        try:\n",
    "            d['iqa_map_path'] = glob.glob(iqa_map_path + ('/**' + n + '**'))[0]\n",
    "        except:\n",
    "            print(\"no directory\")\n",
    "        \n",
    "        try:\n",
    "            s = (m.Winters[-4:] + ' v. ' + m.Winters[0:4]).replace(' ', '_')\n",
    "            iqa_map_path = os.path.join(base_path, 'iqa', s)\n",
    "            d['iqa_map_path'] = glob.glob(iqa_map_path + ('/**' + n + '**'))[0]\n",
    "        except:\n",
    "            print(\"no directory\")\n",
    "\n",
    "        d['snow_map_path1'] = os.path.join(base_path, 'snow_depth', m.Zone + '_snow_depth_' + yr1 + '.tif')\n",
    "        d['snow_map_path2'] = os.path.join(base_path, 'snow_depth', m.Zone + '_snow_depth_' + yr2 + '.tif')\n",
    "        \n",
    "        metric_dicts.append(d)\n",
    "        \n",
    "    return metric_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = make_dict_of_lowest_similarity('aggregate_results.csv')\n",
    "md[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_indices(d):\n",
    "    \n",
    "    src = rasterio.open(d['iqa_map_path'])\n",
    "    argmin = np.where(src.read(1) == np.nanmin(src.read(1)))\n",
    "    y = argmin[0][0]\n",
    "    x = argmin[1][0]\n",
    "    #print(x, y)\n",
    "    return x, y, src.read(1)\n",
    "    \n",
    "def make_transect(x, y, length, bearing):\n",
    "    \n",
    "    # 0 deg. is due East, 90 is S, etc.\n",
    "    cos_a = np.cos((np.deg2rad(bearing)))\n",
    "    cos_b = np.cos((np.deg2rad(90 + bearing)))\n",
    "    \n",
    "    x2 = x + int(length * cos_a)\n",
    "    y2 = y + int(length * cos_b)\n",
    "    \n",
    "    #print(x2, y2)\n",
    "    return x2, y2\n",
    "\n",
    "def make_xsections(d):\n",
    "    \n",
    "    z = rasterio.open(d['dem_path']).read(1)\n",
    "    snow_z1 = rasterio.open(d['snow_map_path1']).read(1) + z\n",
    "    snow_z2 = rasterio.open(d['snow_map_path2']).read(1) + z\n",
    "    \n",
    "    x, y, arr = find_min_indices(d)\n",
    "    x2, y2 = make_transect(x, y, length=40, bearing=0)\n",
    "    \n",
    "    \n",
    "    z_xsection = profile_line(z, (y, x), (y2, x2), linewidth=3)\n",
    "    snowz1_xsection = profile_line(snow_z1, (y, x), (y2, x2), linewidth=3)\n",
    "    snowz2_xsection = profile_line(snow_z2, (y, x), (y2, x2), linewidth=3)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(z_xsection)\n",
    "    plt.plot(snowz1_xsection)\n",
    "    plt.plot(snowz2_xsection)\n",
    "    \n",
    "    return z_xsection, snowz1_xsection, snowz2_xsection\n",
    "    \n",
    "    \n",
    "# for i in md:\n",
    "#     make_xsections(i)\n",
    "    \n",
    "#     x, y, arr = find_min_indices(i)\n",
    "#     x2, y2 = make_transect(x, y, length=40, bearing=0)\n",
    "    \n",
    "#     x_section = profile_line(arr, (y, x), (y2, x2), linewidth=3)\n",
    "#     print(x_section[0])\n",
    "#     print(arr[y, x])\n",
    "#     print('---')\n",
    "#     print(x_section[-1])\n",
    "#     print(arr[y, x+10])\n",
    "#     print('***')\n",
    "#     print(y, x)\n",
    "#     print(x+10)\n",
    "#     print(y2, x2)\n",
    "    \n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.imshow(arr)\n",
    "#     plt.plot(x, y, 'r*', markersize=10)\n",
    "#     plt.plot(x2, y2, 'm^', markersize=10)\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.plot(profile_line(arr, (y, x), (y2, x2)))\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_transect(d, length, bearing):\n",
    "\n",
    "    \n",
    "    \n",
    "    arr = rasterio.open(d['iqa_map_path']).read(1)\n",
    "    \n",
    "    ix_min = np.where(arr == np.nanmin(arr))\n",
    "    ix_min_ls = [i[0] for i in ix_min]\n",
    "    ix_min_tup = tuple(ix_min_ls)\n",
    "    \n",
    "    y1 = ix_min_ls[0]\n",
    "    x1 = ix_min_ls[1]\n",
    "    \n",
    "    cos_a = np.cos((np.deg2rad(bearing)))\n",
    "    cos_b = np.cos((np.deg2rad(90 + bearing)))\n",
    "    \n",
    "    x2 = x1 + int(length * cos_a)\n",
    "    y2 = y1 + int(length * cos_b)\n",
    "    \n",
    "    ix_end = tuple([x2, y2])\n",
    "    \n",
    "    return profile_line(arr, ix_min_tup, ix_end), x1, y1, x2, y2\n",
    "\n",
    "a = min_transect(paths_to_poor_sim(worst_nrmse, 'nrmse'), 1, 0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(d_paths):\n",
    "\n",
    "\n",
    "    src1 = rasterio.open(d_paths['snow_map_path1'])\n",
    "    src2 = rasterio.open(d_paths['snow_map_path2'])\n",
    "    src3 = rasterio.open(d_paths['iqa_map_path'])\n",
    "    src4 = rasterio.open(d_paths['dem_path'])\n",
    "\n",
    "    sd1 = src1.read(1)\n",
    "    sd2 = src2.read(1)\n",
    "    iqa = src3.read(1)\n",
    "    dem = src4.read(1)\n",
    "    \n",
    "    ss1 = dem + sd1\n",
    "    ss2 = dem + sd2\n",
    "    \n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    f, axes = plt.subplots(2, 2, figsize=(16,10))\n",
    "    \n",
    "    axes[0][0].imshow(sd1, vmin=0, vmax=1, cmap='viridis')\n",
    "    axes[0][0].set_title(d_paths['snow_map_path1'].split('_')[-1].rstrip('.tif') + ' Snow Depth [m]')\n",
    "    \n",
    "    axes[0][1].imshow(sd2, vmin=0, vmax=1, cmap='viridis')\n",
    "    axes[0][1].set_title(d_paths['snow_map_path2'].split('_')[-1].rstrip('.tif') + ' Snow Depth [m]')\n",
    "    \n",
    "    axes[1][0].imshow(iqa, vmin=0.5, vmax=1, cmap='coolwarm_r')\n",
    "    axes[1][0].set_title(d_paths['iqa_map_path'].split('_')[-2].upper())\n",
    "    \n",
    "    pro, x1, y1, x2, y2 = min_transect(d_paths, 10, 0)\n",
    "    \n",
    "    print(x1, y1, x2, y2)\n",
    "    \n",
    "    axes[1][0].arrow(x1, y1, x2, y2, fc=\"m\", ec=\"k\",\n",
    "                    head_width=0.05, head_length=0.1 )\n",
    "    \n",
    "    #axes[1][0].plot(x1, y1, x2, y2, marker = 'o', ls='-', lw=3, c='m')\n",
    "    \n",
    "    axes[1][1].set_title('Surfaces')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity(paths_to_poor_sim(worst_nrmse, 'nrmse'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity(paths_to_poor_sim(worst_cwssim, 'cwssim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity(paths_to_poor_sim(worst_nrmse, 'nrmse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/lib64/libpango-1.0.so.0: undefined symbol: g_log_structured_standard",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8abddaf03146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexposure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../clpx_lake_e/raster/snow_depth/clpx_lake_e_snow_depth_2015.tif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: /lib64/libpango-1.0.so.0: undefined symbol: g_log_structured_standard"
     ]
    }
   ],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "src = rasterio.open('../clpx_lake_e/raster/snow_depth/clpx_lake_e_snow_depth_2015.tif')\n",
    "image = src.read(1)\n",
    "\n",
    "# fd, hog_image = hog(image, visualize=True)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "# ax1.axis('off')\n",
    "# ax1.imshow(image, cmap=plt.cm.gray)\n",
    "# ax1.set_title('Input image')\n",
    "\n",
    "# # Rescale histogram for better display\n",
    "# #hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "# ax2.axis('off')\n",
    "# ax2.imshow(hog_image, cmap=plt.cm.gray)\n",
    "# ax2.set_title('Histogram of Oriented Gradients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fd)\n",
    "#fd\n",
    "#plt.hist(np.rad2deg(fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hog_image.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Skip to content\n",
    "Pull requests\n",
    "Issues\n",
    "Marketplace\n",
    "Explore\n",
    "@charparr\n",
    "\n",
    "1\n",
    "0\n",
    "\n",
    "    0\n",
    "\n",
    "charparr/tabler\n",
    "Code\n",
    "Issues 0\n",
    "Pull requests 0\n",
    "Projects 0\n",
    "Wiki\n",
    "Insights\n",
    "Settings\n",
    "tabler/tabler_vectors.py\n",
    "Charlie Parr Snowdrift Measurements, Tabler Vectors, Benson Flux in Python d6dcc5a on Jun 1, 2017\n",
    "900 lines (773 sloc) 33.9 KB\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import warnings\n",
    "#from math import factorial\n",
    "from skimage import io\n",
    "from skimage.measure import profile_line\n",
    "#from scipy.ndimage import gaussian_filter\n",
    "from scipy import integrate\n",
    "import scipy.ndimage\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "class Surface(object):\n",
    "    '''A class for bare earth and snow surfaces.\n",
    "    Each instance is initialized to mask out common NoData values and remove\n",
    "    empty rows. The surface is also resampled to 1 by 1 meter data to make\n",
    "    profile and flux calculations more straightforward.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, fpath):\n",
    "        self.arr = io.imread(fpath)\n",
    "        self.arr[self.arr == -9999] = np.nan\n",
    "        self.arr[(self.arr < -10)] = np.nan\n",
    "        rowmask = np.all(np.isnan(self.arr), axis=1)\n",
    "        self.arr = self.arr[~rowmask]\n",
    "        self.arr = scipy.ndimage.zoom(self.arr, 2, order = 1)\n",
    "        \n",
    "    def subset_surf(self, n_divs, names):\n",
    "        '''Subset the surface remove empty columns. Subsets are stored in a \n",
    "        dict with user defined keys.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_divs : int\n",
    "            the number of vertical subsets.\n",
    "        names: list of strings for keys\n",
    "        '''\n",
    "        ysize = round(self.arr.shape[0] / n_divs)\n",
    "        \n",
    "        self.subdict = dict.fromkeys(names)\n",
    "        \n",
    "        for nam, num in zip(names, range(0, n_divs)):\n",
    "            self.subdict[nam] = self.arr[(ysize * num):(ysize * (num+1))]\n",
    "            \n",
    "            colmask = np.nansum(self.subdict[nam],axis=0)==0\n",
    "            emptycols = np.where(colmask)\n",
    "            nd_boundaries = [j-i for i, j in zip(emptycols[0][:-1], emptycols[0][1:])]\n",
    "    \n",
    "            if max(nd_boundaries, default='no elements') == 1:\n",
    "                \n",
    "                if emptycols[0][0]==0:\n",
    "                    \n",
    "                    self.subdict[nam] = self.subdict[nam][::, emptycols[0].max():]\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    self.subdict[nam] = self.subdict[nam][::, :emptycols[0].min()]\n",
    "            \n",
    "            elif max(nd_boundaries, default='no elements') == 'no elements':\n",
    "                \n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                l_edge = np.where(np.array(nd_boundaries) > 1)[0][0]\n",
    "                r_edge = l_edge + max(nd_boundaries)\n",
    "                self.subdict[nam] = self.subdict[nam][::,l_edge:r_edge]\n",
    "        \n",
    "class Transect(object):\n",
    "    '''A class for transects along which we can measure flux and generate\n",
    "    Tabler profiles. We initialize a Transect instance by providing the bare\n",
    "    earth snow depth surfaces and the beginning and end indices for\n",
    "    the profile we want. We can initialize Tabler assuming the user is choosing\n",
    "    the start and end of the drift as the profile.\n",
    "        Parameters\n",
    "        ----------\n",
    "        bare_earth : array_like, shape (N,)\n",
    "            elevation values of snow free surface\n",
    "        years : list[str]\n",
    "            years for which snow depth surfaces are available\n",
    "        snow_depths : list[arr]\n",
    "            snow depth surfaces\n",
    "        angle_deg: int\n",
    "            angle in degrees TN from which we take the transect (i.e. the\n",
    "            the wind direction)\n",
    "        x1, y1: int\n",
    "            start indices of transect\n",
    "        length: int\n",
    "            length of transect in meters\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, bare_earth, years, snow_depths, angle_deg, x1, y1, length):\n",
    "        \n",
    "        self.bare_earth = bare_earth # bare earth surface\n",
    "        self.angle_deg = angle_deg # where does the wind come from?\n",
    "        self.x1 = x1\n",
    "        self.y1 = y1\n",
    "        \n",
    "        # Compute end of transect\n",
    "        cosa = np.cos((np.deg2rad(90 + angle_deg)))\n",
    "        cosb = np.cos((np.deg2rad(angle_deg)))\n",
    "        self.x2, self.y2 = (x1 +(length * cosa), y1+(length * cosb))\n",
    "            \n",
    "        # Construct DEM profile with a width of 1 m.\n",
    "        self.dem_profile = profile_line(bare_earth,\n",
    "                                               ((y1,x1)),((self.y2,self.x2)),1)\n",
    "        \n",
    "        # Check DEM profile for No Data values and where they occur.\n",
    "        nan_idx = np.argwhere(np.isnan(self.dem_profile))\n",
    "        \n",
    "        # Truncate DEM Profile at the first No Data instance\n",
    "        if len(nan_idx) > 0:\n",
    "            \n",
    "            self.dem_profile = self.dem_profile[:nan_idx[0]-1]\n",
    "        \n",
    "        # The DEM profile is a template for the snow depth profile\n",
    "        \n",
    "        dem_pro_len = len(self.dem_profile)\n",
    "\n",
    "        self.x3, self.y3 = (x1 +(dem_pro_len * cosa), y1+(dem_pro_len * cosb))\n",
    "       \n",
    "        self.snowdict = dict.fromkeys(years)\n",
    "        \n",
    "        # For each year generate profiles and store in dictionary\n",
    "        for yr, depth in zip(years, snow_depths):\n",
    "            \n",
    "            self.snowdict[yr] = {}\n",
    "            \n",
    "            self.snowdict[yr]['snow depth surface'] = depth\n",
    "            self.snowdict[yr]['winter surface'] = depth + self.bare_earth\n",
    "            \n",
    "            self.snowdict[yr]['winter surface profile'] = \\\n",
    "                         profile_line(self.snowdict[yr]['winter surface'],\n",
    "                                      ((y1,x1)),((self.y3,self.x3)),1)[:-1]\n",
    "                         \n",
    "            self.snowdict[yr]['depth profile'] = profile_line(depth,((y1,x1)),\n",
    "                         ((self.y3,self.x3)),1)[:-1]\n",
    "        \n",
    "    def TablerProfile(self):\n",
    "        \n",
    "        '''\n",
    "        Here we generate a Tabler Equilibirum Profile for the drift.\n",
    "        Parameters include the lip of the trap and the drift end, because the\n",
    "        we need to have more data (e.g. upwind) in the transect. Basically the\n",
    "        user decides where the drift starts and ends. I have ways to automate\n",
    "        this but they are not fully tested.\n",
    "        '''\n",
    "        # Dynamic Creation of Tabler Profile\n",
    "        # All Coeffcients march downwind 1 m at a time\n",
    "        # x2 is now the slope from the snow surface to the ground at a\n",
    "        # horizontal distance of 15 m downwind.\n",
    "        \n",
    "        self.dynamic_tabler = self.dem_profile.copy()\n",
    "        \n",
    "        i = 45\n",
    "        while i < len(self.dem_profile) - 46:\n",
    "            \n",
    "            upwind_0_45 = self.dem_profile[i-45] - self.dem_profile[i]\n",
    "                   \n",
    "            snow_to_ground = self.dynamic_tabler[i] - self.dem_profile[i+5]\n",
    "                   \n",
    "            downwind_15_30 = self.dem_profile[i+15] - self.dem_profile[i+30]\n",
    "                   \n",
    "            downwind_30_45 = self.dem_profile[i+30] - self.dem_profile[i+45]\n",
    "            \n",
    "            if upwind_0_45 > 0:\n",
    "                x1 = (upwind_0_45 / 45) * -100\n",
    "            else:\n",
    "                x1 = (upwind_0_45 / 45) * 100\n",
    "                     \n",
    "            if snow_to_ground > 0:\n",
    "                x2 = snow_to_ground * -100\n",
    "            else:\n",
    "                x2 = snow_to_ground * 100                    \n",
    "                    \n",
    "            if downwind_15_30 > 0:\n",
    "                x3 = (downwind_15_30 / 15) * -100\n",
    "            else:\n",
    "                x3 = (downwind_15_30 / 15) * 100\n",
    "                    \n",
    "            if downwind_30_45 > 0:\n",
    "                x4 = (downwind_30_45 / 15) * -100\n",
    "            else:\n",
    "                x4 = (downwind_30_45 / 15) * 100\n",
    "                     \n",
    "            y = (0.25 * x1) + (0.55 * x2) + (0.15 * x3) + (0.05 * x4)\n",
    "            \n",
    "            rise = y / 100\n",
    "            \n",
    "            self.dynamic_tabler[i+1] = self.dem_profile[i] - rise\n",
    "                               \n",
    "            i+=1\n",
    "            \n",
    "    def Compute_Flux(self):\n",
    "         ''' Use Simpson's rule integration to calculate flux over the drift\n",
    "         fetch. Values will be m^3 per lineal m, so essentialy we have m^2 which\n",
    "         makes sense because we are getting the area under the snow depth curve.\n",
    "         '''\n",
    "         \n",
    "         tabler_snow_depth = self.dynamic_tabler - self.dem_profile\n",
    "         self.tabler_flux = integrate.simps(tabler_snow_depth[46:-46])                 \n",
    "                                                \n",
    "         self.mean_flux = 0\n",
    "         for k in self.snowdict:\n",
    "             \n",
    "            snow_depth = self.snowdict[k]['depth profile']\n",
    "            self.snowdict[k]['flux'] = integrate.simps(snow_depth[46:-46])\n",
    "            self.mean_flux = self.mean_flux + self.snowdict[k]['flux']\n",
    "        \n",
    "         self.mean_flux = self.mean_flux / 4.0\n",
    "         self.tabler_flux_err = self.mean_flux - self.tabler_flux\n",
    "         self.flux_err_ratio = self.tabler_flux_err / self.tabler_flux\n",
    "\n",
    "            \n",
    "                                                                            \n",
    "    def PlotMaps(self):\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax1=plt.subplot(2,2,1)\n",
    "        im1=plt.imshow(self.snowdict['2016']['snow depth surface'],\n",
    "                       vmin = 0, vmax = 2)\n",
    "        ax1.plot([self.x1, self.x3], [self.y1, self.y3], c='r',\n",
    "                 linestyle='-', linewidth=2)\n",
    "        plt.title('2016')\n",
    "        ax1.set_ylabel('m',size = 7)\n",
    "\n",
    "        \n",
    "        ax2=plt.subplot(2,2,2,sharex=ax1)\n",
    "        plt.imshow(self.snowdict['2015']['snow depth surface'],\n",
    "                   vmin = 0, vmax = 2)\n",
    "        ax2.plot([self.x1, self.x3], [self.y1, self.y3], c='r',\n",
    "                 linestyle='-', linewidth=2)\n",
    "        plt.title('2015')\n",
    "        ax2.set_yticks([])\n",
    "        ax2.set_xticks([])\n",
    "        \n",
    "        ax3=plt.subplot(2,2,3,sharey=ax1)\n",
    "        plt.imshow(self.snowdict['2013']['snow depth surface'],\n",
    "                   vmin = 0, vmax = 2)\n",
    "        ax3.plot([self.x1, self.x3], [self.y1, self.y3], c='r',\n",
    "                 linestyle='-', linewidth=2)\n",
    "        plt.title('2013')\n",
    "        ax3.set_xlabel('m',size = 7)\n",
    "\n",
    "        ax4=plt.subplot(2,2,4, sharex = ax3)\n",
    "        plt.imshow(self.snowdict['2012']['snow depth surface'],\n",
    "                   vmin = 0, vmax = 2)\n",
    "        ax4.plot([self.x1, self.x3], [self.y1, self.y3], c='r',\n",
    "                 linestyle='-', linewidth=2)\n",
    "        plt.title('2012')\n",
    "        ax4.set_yticks([])\n",
    "        ax4.set_xlabel('m',size = 7)\n",
    "        \n",
    "        cbar_ax = fig.add_axes([0.90, 0.15, 0.05, 0.7])\n",
    "        fig.colorbar(im1, cax=cbar_ax)\n",
    "        plt.suptitle('Snow Depth [m]')\n",
    "        plt.subplots_adjust(wspace=0.0,hspace=0.2)\n",
    "        \n",
    "        #plt.savefig('/home/cparr/depthmaps.png',dpi=300)\n",
    "\n",
    "                \n",
    "    def PlotDepthProfiles(self):\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(self.snowdict['2012']['depth profile'], ':b1', label = '2012')\n",
    "        plt.plot(self.snowdict['2013']['depth profile'], ':r2', label = '2013')\n",
    "        plt.plot(self.snowdict['2015']['depth profile'], ':g3', label = '2015')\n",
    "        plt.plot(self.snowdict['2016']['depth profile'], ':k4', label = '2016')\n",
    "        plt.ylabel('Snow Depth [m]')\n",
    "        plt.xlabel('m')\n",
    "        plt.legend()\n",
    "        #plt.savefig('/home/cparr/snowdepth_profiles.png',dpi=300)\n",
    "\n",
    "        \n",
    "    def PlotSurfaceProfiles(self):\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(self.dem_profile, label = 'Bare Earth',c='saddlebrown')\n",
    "        plt.plot(self.snowdict['2012']['winter surface profile'], '--b',alpha=0.5, label = '2012')\n",
    "        plt.plot(self.snowdict['2013']['winter surface profile'], '--r',alpha=0.5, label = '2013')\n",
    "        plt.plot(self.snowdict['2015']['winter surface profile'], '--g',alpha=0.5, label = '2015')\n",
    "        plt.plot(self.snowdict['2016']['winter surface profile'], '--m',alpha=0.5, label = '2016')\n",
    "        plt.plot(self.dynamic_tabler, label = 'Dynamic Tabler Surface',c='k')\n",
    "        plt.xlabel('m')\n",
    "        plt.ylabel('m')                \n",
    "        plt.legend()\n",
    "\n",
    "    def PlotSnowSurfacesOnly(self):\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(self.snowdict['2012']['winter surface profile'] - self.dem_profile, '--b',alpha=0.5, label = '2012')\n",
    "        plt.plot(self.snowdict['2013']['winter surface profile'] - self.dem_profile, '--r',alpha=0.5, label = '2013')\n",
    "        plt.plot(self.snowdict['2015']['winter surface profile'] - self.dem_profile, '--g',alpha=0.5, label = '2015')\n",
    "        plt.plot(self.snowdict['2016']['winter surface profile'] - self.dem_profile, '--m',alpha=0.5, label = '2016')\n",
    "        plt.plot(self.dynamic_tabler - self.dem_profile, label = 'Dynamic Tabler Surface',c='k')\n",
    "        plt.xlabel('m')\n",
    "        plt.ylabel('m')\n",
    "        plt.title('Snow Surface - Bare Earth Surface')                \n",
    "        plt.legend()\n",
    "\n",
    "    def PlotFlux(self):\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.suptitle('Integrated Flux [m^3 per lineal m]')\n",
    "        y = [self.snowdict['2012']['flux'], \n",
    "                self.snowdict['2013']['flux'],\n",
    "                self.snowdict['2015']['flux'],\n",
    "                self.snowdict['2016']['flux'],\n",
    "                self.tabler_flux]\n",
    "        ax.bar(np.arange(5),y)\n",
    "        ax.set_xticklabels(('','2012','2013','2015','2016','Tabler'))\n",
    "        ax.set_xlabel('Profile')\n",
    "        ax.set_ylabel('Flux')\n",
    "\n",
    "\n",
    "    def PlotAll(self):\n",
    "        self.PlotMaps()\n",
    "        self.PlotDepthProfiles()\n",
    "        self.PlotSurfaceProfiles()\n",
    "        self.PlotSnowSurfacesOnly()\n",
    "        self.PlotFlux()\n",
    "\n",
    "def make_transect_obj(bare, years, snows, angle, x1,y1,length):\n",
    "    t = Transect(bare, years, snows, angle, x1,y1,length)\n",
    "    t.TablerProfile()\n",
    "    t.Compute_Flux()\n",
    "    return t\n",
    "        \n",
    "### globally bring in surfaces and test transects\n",
    "\n",
    "years = ['2012','2013','2015','2016']\n",
    "  \n",
    "bare_surf = Surface('/home/cparr/surfaces/level_1_surfaces/hv/bare_earth/hv_2012_158_bare_earth_dem.tif')\n",
    "snow_depth_surf16 = Surface('/home/cparr/surfaces/depth_ddems/hv/hv_2016_096_depth.tif')\n",
    "snow_depth_surf15 = Surface('/home/cparr/surfaces/depth_ddems/hv/hv_2015_096_depth.tif')\n",
    "snow_depth_surf13 = Surface('/home/cparr/surfaces/depth_ddems/hv/hv_2013_103_depth.tif')\n",
    "snow_depth_surf12 = Surface('/home/cparr/surfaces/depth_ddems/hv/hv_2012_107_depth.tif')\n",
    "\n",
    "snow_depth_surf16.arr = snow_depth_surf16.arr[2:]\n",
    "snow_depth_surf15.arr = snow_depth_surf15.arr[2:]\n",
    "snow_depth_surf13.arr = snow_depth_surf13.arr[2:]\n",
    "bare_surf.arr = bare_surf.arr[2:]\n",
    "\n",
    "bare_surf.subset_surf(12, ['b1','b2','b3','b4',\n",
    "                           'b5','b6','b7','b8',\n",
    "                           'b9','b10','b11','b12'])\n",
    "    \n",
    "snow_depth_surf16.subset_surf(12, ['2016_1','2016_2','2016_3','2016_4',\n",
    "                                 '2016_5','2016_6','2016_7','2016_8',\n",
    "                                 '2016_9','2016_10','2016_11','2016_12'])\n",
    "    \n",
    "snow_depth_surf15.subset_surf(12, ['2015_1','2015_2','2015_3','2015_4',\n",
    "                                 '2015_5','2015_6','2015_7','2015_8',\n",
    "                                 '2015_9','2015_10','2015_11','2015_12'])\n",
    "    \n",
    "snow_depth_surf13.subset_surf(12, ['2013_1','2013_2','2013_3','2013_4',\n",
    "                                 '2013_5','2013_6','2013_7','2013_8',\n",
    "                                 '2013_9','2013_10','2013_11','2013_12'])\n",
    "    \n",
    "snow_depth_surf12.subset_surf(12, ['2012_1','2012_2','2012_3','2012_4',\n",
    "                                 '2012_5','2012_6','2012_7','2012_8',\n",
    "                                 '2012_9','2012_10','2012_11','2012_12'])\n",
    "            \n",
    "\n",
    "#bare_surf.subset_surf(4, ['b1','b2','b3','b4'])\n",
    "#    \n",
    "#snow_depth_surf16.subset_surf(4, ['2016_1','2016_2','2016_3','2016_4'])\n",
    "#    \n",
    "#snow_depth_surf15.subset_surf(4, ['2015_1','2015_2','2015_3','2015_4'])\n",
    "#    \n",
    "#snow_depth_surf13.subset_surf(4, ['2013_1','2013_2','2013_3','2013_4'])\n",
    "#    \n",
    "#snow_depth_surf12.subset_surf(4, ['2012_1','2012_2','2012_3','2012_4'])\n",
    "    \n",
    "    \n",
    "\n",
    "kidney_lake_bare = bare_surf.subdict['b7']\n",
    "kidney_lake_snows = [snow_depth_surf12.subdict['2012_7'],\n",
    "                    snow_depth_surf13.subdict['2013_7'],\n",
    "                    snow_depth_surf15.subdict['2015_7'],\n",
    "                    snow_depth_surf16.subdict['2016_7']]\n",
    "\n",
    "\n",
    "#test = make_transect_obj(kidney_lake_bare, years, kidney_lake_snows, 270,70,750,200)\n",
    "#test.PlotAll()\n",
    "\n",
    "###\n",
    "\n",
    "class Flux_point(object):\n",
    "    '''A class for flux points where we determine the seasonal flux direction\n",
    "    and flux amount.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, x,y,name,windrange):\n",
    "        \n",
    "        self.results = dict()\n",
    "        self.df = pd.DataFrame()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.name = name\n",
    "        self.transect_keys = []\n",
    "        self.windrange = np.arange(windrange[0],windrange[1],windrange[2])\n",
    "        \n",
    "        \n",
    "        for w in self.windrange:\n",
    "            self.transect_keys.append(self.name + '_' + str(w))\n",
    "                    \n",
    "        for k in zip(self.transect_keys, self.windrange):\n",
    "            self.results[k[0]] = make_transect_obj(kidney_lake_bare,\n",
    "                                years,\n",
    "                                kidney_lake_snows, k[1],x,y,200) # 200 m long.\n",
    "        \n",
    "        idx = []\n",
    "        tflux = []\n",
    "        fluxerr = []\n",
    "        fluxerrratio = []\n",
    "        angle = []\n",
    "    \n",
    "        for k in self.results.items():\n",
    "            idx.append(str(k[0]))\n",
    "            angle.append(k[1].angle_deg)\n",
    "            fluxerr.append(k[1].tabler_flux_err)\n",
    "            tflux.append(k[1].tabler_flux)\n",
    "            fluxerrratio.append(k[1].flux_err_ratio)\n",
    "        \n",
    "        self.df['idx']=idx\n",
    "        self.df['wind direction'] = angle\n",
    "        self.df['tabler flux error'] = fluxerr\n",
    "        self.df['abs flux error'] = abs(self.df['tabler flux error'])\n",
    "        self.df['tabler flux'] = tflux\n",
    "        self.df['y'] = y\n",
    "        self.df['err_ratio'] = fluxerrratio\n",
    "        self.df['x']=x\n",
    "        self.df.sort_values(['abs flux error'],inplace=True)\n",
    "               \n",
    "        tabler_match1 = self.df.iloc[0]\n",
    "        tabler_match2 = self.df.iloc[1]\n",
    "        tabler_match3 = self.df.iloc[2]\n",
    "#        tabler_match1 = self.df.ix[np.argmin(abs(self.df['err_ratio']))]\n",
    "        self.tabler_vector1 = tabler_match1['wind direction']\n",
    "        self.tabler_vector2 = tabler_match2['wind direction']\n",
    "        self.tabler_vector3 = tabler_match3['wind direction']\n",
    "\n",
    "        \n",
    "def plot_flux(points):\n",
    "    '''\n",
    "    Plot the profile lines on top of one snow depth map.\n",
    "    Lines should fan out from each point at every angle specified.\n",
    "    '''\n",
    "    fig=plt.figure(figsize=(8,5))\n",
    "    ax1=plt.subplot(1,1,1)\n",
    "    im1=plt.imshow(kidney_lake_snows[2], cmap='viridis', vmin = 0, vmax = 2)\n",
    "    ax1.set_ylabel('m',size = 7)\n",
    "    ax1.set_xlabel('m',size = 7)\n",
    "    plt.title('2015 Snow Depth Map [m]')                                            \n",
    "    cbar_ax = fig.add_axes([0.90, 0.15, 0.05, 0.7])\n",
    "    fig.colorbar(im1, cax=cbar_ax)\n",
    "\n",
    "    for p in points:\n",
    "        for k in p.results:\n",
    "            ax1.plot([p.results[k].x1, p.results[k].x3],\n",
    "                     [p.results[k].y1, p.results[k].y3], c='r',\n",
    "                     alpha=0.5, linestyle='-', linewidth=1)\n",
    "    #plt.savefig('/home/cparr/Snow_Patterns/figures/tabler_test.png',dpi=450)\n",
    "        \n",
    "def flux_map(points):\n",
    "    '''\n",
    "    Plot the profile lines on top of one snow depth map.\n",
    "    Lines should fan out from each point at every angle specified.\n",
    "    '''\n",
    "    fig=plt.figure(figsize=(8,5))\n",
    "    ax1=plt.subplot(1,1,1)\n",
    "    im1=plt.imshow(kidney_lake_snows[2], cmap='viridis', vmin = 0, vmax = 2)\n",
    "    ax1.set_ylabel('m',size = 7)\n",
    "    ax1.set_xlabel('m',size = 7)\n",
    "    plt.title('2015 Snow Depth Map [m]')                                            \n",
    "    cbar_ax = fig.add_axes([0.90, 0.15, 0.05, 0.7])\n",
    "    fig.colorbar(im1, cax=cbar_ax)\n",
    "\n",
    "    for p in points:\n",
    "        for k in p.results:\n",
    "            if p.results[k].angle_deg == p.tabler_vector1 or \\\n",
    "                p.results[k].angle_deg == p.tabler_vector2 or \\\n",
    "                p.results[k].angle_deg == p.tabler_vector3:\n",
    "                \n",
    "                if p.results[k].angle_deg == p.tabler_vector1:\n",
    "                    width=7.0\n",
    "                    fc = 'r'\n",
    "                elif p.results[k].angle_deg == p.tabler_vector2:\n",
    "                    width=5.0\n",
    "                    fc = 'y'\n",
    "                else:\n",
    "                    width = 3.0\n",
    "                    fc='white'\n",
    "                        \n",
    "                ax1.arrow(\n",
    "                        p.results[k].x1,            # x\n",
    "                        p.results[k].y1,            # y\n",
    "                        p.results[k].x3 - p.results[k].x1,            # dx\n",
    "                        p.results[k].y3 - p.results[k].y1,            # dy\n",
    "                        length_includes_head = True,\n",
    "                        width = width,\n",
    "                        fc = fc\n",
    "\n",
    "                    )\n",
    "#                ax1.annotate(str(int(p.results[k].mean_flux)),\n",
    "#                             xy=(p.results[k].x1, p.results[k].y1),\n",
    "#                                xycoords=\"data\",\n",
    "#                  va=\"center\", ha=\"center\",\n",
    "#                  bbox=dict(boxstyle=\"round\", fc=\"w\"))\n",
    "    #plt.savefig('/home/cparr/Snow_Patterns/figures/tabler_test.png',dpi=450)\n",
    "\n",
    "\n",
    "lake_n = Flux_point(125,650,'lake_n',[180,370,15])\n",
    "lake_w = Flux_point(80,750,'lake_w',[180,370,15])\n",
    "lake_s = Flux_point(90,850,'lake_s',[180,370,15])\n",
    "lake_n1 = Flux_point(135,670,'lake_n1',[180,370,15])\n",
    "lake_w1 = Flux_point(90,770,'lake_w1',[180,370,15])\n",
    "lake_s1 = Flux_point(100,870,'lake_s1',[180,370,15])\n",
    "\n",
    "gully1_n = Flux_point(80,200,'gully1_n',[180,370,15])\n",
    "gully1_w = Flux_point(80,250,'gully1_w',[180,370,15])\n",
    "gully1_s = Flux_point(80,300,'gully1_s',[180,370,15])\n",
    "gully1_n1 = Flux_point(95,200,'gully1_n1',[180,370,15])\n",
    "gully1_w1 = Flux_point(95,250,'gully1_w1',[180,370,15])\n",
    "gully1_s1 = Flux_point(110,300,'gully1_s1',[180,370,15])\n",
    "gully1_n2 = Flux_point(110,200,'gully1_n2',[180,370,15])\n",
    "gully1_w2 = Flux_point(110,250,'gully1_w2',[180,370,15])\n",
    "gully1_s2 = Flux_point(110,300,'gully1_s2',[180,370,15])\n",
    "\n",
    "gully2_n = Flux_point(310,50,'gully2_n',[180,370,15])\n",
    "gully2_w = Flux_point(310,100,'gully2_w',[180,370,15])\n",
    "gully2_s = Flux_point(310,150,'gully2_s',[180,370,15])\n",
    "gully2_n1 = Flux_point(345,50,'gully2_n1',[180,370,15])\n",
    "gully2_w1 = Flux_point(345,100,'gully2_w1',[180,370,15])\n",
    "gully2_s1 = Flux_point(345,150,'gully2_s1',[180,370,15])\n",
    "\n",
    "gully3_n = Flux_point(1100,250,'gully3_n',[180,370,15])\n",
    "gully3_w = Flux_point(1100,300,'gully3_w',[180,370,15])\n",
    "gully3_s = Flux_point(1050,350,'gully3_s',[180,370,15])\n",
    "gully3_n1 = Flux_point(1100,280,'gully3_n1',[180,370,15])\n",
    "gully3_w1 = Flux_point(1100,330,'gully3_w1',[180,370,15])\n",
    "gully3_s1 = Flux_point(1050,380,'gully3_s1',[180,370,15])\n",
    "gully3_n2 = Flux_point(1000,400,'gully3_n2',[180,370,15])\n",
    "gully3_w2 = Flux_point(1000,415,'gully3_w2',[180,370,15])\n",
    "gully3_s2 = Flux_point(1000,430,'gully3_s2',[180,370,15])\n",
    "gully3_n3 = Flux_point(1075,500,'gully3_n3',[180,370,15])\n",
    "gully3_w3 = Flux_point(1100,500,'gully3_w3',[180,370,15])\n",
    "gully3_s3 = Flux_point(1125,500,'gully3_s3',[180,370,15])\n",
    "gully3_n4 = Flux_point(1025,450,'gully3_n4',[180,370,15])\n",
    "gully3_w4 = Flux_point(1050,450,'gully3_w4',[180,370,15])\n",
    "gully3_s4 = Flux_point(1075,450,'gully3_s4',[180,370,15])\n",
    "\n",
    "flux_pts = [lake_n,lake_w,lake_s,\n",
    "            lake_n1,lake_w1,lake_s1,\n",
    "          gully1_n,gully1_w,gully1_s,\n",
    "          gully1_n1,gully1_w1,gully1_s1,\n",
    "          gully1_n2,gully1_w2,gully1_s2,\n",
    "          gully2_n,gully2_w,gully2_s,\n",
    "          gully2_n1,gully2_w1,gully2_s1,\n",
    "          gully3_n,gully3_w,gully3_s,\n",
    "          gully3_n1,gully3_w1,gully3_s1,\n",
    "          gully3_n2,gully3_w2,gully3_s2,\n",
    "          gully3_n3,gully3_w3,gully3_s3,\n",
    "          gully3_n4,gully3_w4,gully3_s4]\n",
    "\n",
    "#plot_flux(flux_pts)\n",
    "    \n",
    "flux_map(flux_pts)\n",
    "\n",
    "#plt.figure()\n",
    "#sns.barplot(x=gully3_w4.df['wind direction'],y=gully3_w4.df['flux err. / tabler flux'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### everything below here is trying to follow the curve of the lake\n",
    "#nd_limits = []\n",
    "#for k in kidney_lake_results:\n",
    "#    if kidney_lake_results[k].angle_deg == 290:\n",
    "#        nd_limits.append(kidney_lake_results[k].no_data_limit)\n",
    "#\n",
    "##limit of lake\n",
    "#\n",
    "#curve_list = []\n",
    "#ylocs = []\n",
    "#xlocs = []\n",
    "#\n",
    "#i = 0\n",
    "#\n",
    "#for l in kidney_lake_bare:\n",
    "#    \n",
    "#    \n",
    "#    nan_idx = np.argwhere(np.isnan(l[50:]))\n",
    "#\n",
    "#    if len(nan_idx) > 0:\n",
    "#        c = min(nan_idx)\n",
    "#        if c < 150:\n",
    "#            ylocs.append(i)\n",
    "#            curve_list.append(int(c))\n",
    "#    i += 1\n",
    "#\n",
    "#for c in curve_list:\n",
    "#    xlocs.append(c-50)\n",
    "#    \n",
    "#\n",
    "#    \n",
    "#plt.plot(curve_list)\n",
    "#plt.plot(xlocs)\n",
    "#plt.plot(ylocs)\n",
    "## test using lake buffer as start points\n",
    "#\n",
    "#kidney_lake_results = dict()\n",
    "#df = pd.DataFrame()\n",
    "#\n",
    "#def test_wind_dirs(dir1,dir2,step):\n",
    "#\n",
    "#    profile_keys = []\n",
    "#    wind_dirs = []\n",
    "#    \n",
    "#    for n in np.arange(dir1,dir2,step):\n",
    "#        for i in range(1,12):\n",
    "#            profile_keys.append('Lake_'+str(n)+'_'+str(i))\n",
    "#            wind_dirs.append(n)\n",
    "# \n",
    "#    start_xs = [n for n in xlocs]*len(profile_keys)\n",
    "#    start_ys = [n for n in np.arange(min(ylocs),max(ylocs),1)]*len(profile_keys)\n",
    "#\n",
    "#\n",
    "#    \n",
    "#    for k in zip(profile_keys, start_xs, start_ys, wind_dirs):\n",
    "#        kidney_lake_results[k[0]] = make_transect_obj(kidney_lake_bare,\n",
    "#                            years,\n",
    "#                            kidney_lake_snows,\n",
    "#                            k[3],k[1],k[2],200,50,106)\n",
    "#        \n",
    "#    idx = []\n",
    "#    tflux = []\n",
    "#    fluxerr = []\n",
    "#    angle = []\n",
    "#    ystart = []\n",
    "#    \n",
    "#    for k in kidney_lake_results.items():\n",
    "#        idx.append(str(k[0]))\n",
    "#        angle.append(k[1].angle_deg)\n",
    "#        fluxerr.append(k[1].tabler_flux_err)\n",
    "#        tflux.append(k[1].tabler_flux)\n",
    "#        ystart.append(k[1].y1)\n",
    "#        \n",
    "#    df['idx']=idx\n",
    "#    df['wind direction'] = angle\n",
    "#    df['tabler flux error'] = fluxerr\n",
    "#    df['tabler flux'] = tflux\n",
    "#    df['y1'] = ystart\n",
    "#        \n",
    "#test_wind_dirs(260,305,5)\n",
    "#\n",
    "#plot_all_one_map()\n",
    "#wt0 = Transect(watertrack_lake_bare,years,watertrack_lake_snows, 300,200,300,250)\n",
    "#wt0.TablerProfile(23,48, wt0.snowdict['2012']['winter surface profile'], 8)\n",
    "#big_lake0 = Transect(big_lake_bare,years,big_lake_snows, 300,40,340,120)\n",
    "#big_lake0.TablerProfile(30,70, big_lake0.snowdict['2012']['winter surface profile'], 8)\n",
    "\n",
    "#==============================================================================\n",
    "#  \n",
    "#     def savitzky_golay(self, y, window_size, order, deriv=0, rate=1):\n",
    "#         \"\"\"Smooth (and optionally differentiate) with a Savitzky-Golay filter.\n",
    "#         The filter removes high frequency noise from data.\n",
    "#         It has the advantage of preserving the original shape and\n",
    "#         features of the signal better than other types of filtering\n",
    "#         approaches, such as moving averages techniques.\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         y : array_like, shape (N,)\n",
    "#             the values of the time history of the signal.\n",
    "#         window_size : int\n",
    "#             the length of the window. Must be an odd integer number.\n",
    "#         order : int\n",
    "#             the order of the polynomial used in the filtering.\n",
    "#             Must be less then `window_size` - 1.\n",
    "#         deriv: int\n",
    "#             order of the derivative to compute (default = 0 is only smoothing)\n",
    "#         Returns\n",
    "#         -------\n",
    "#         ys : ndarray, shape (N)\n",
    "#             the smoothed signal (or it's n-th derivative).\n",
    "#         Notes\n",
    "#         -----\n",
    "#         The Savitzky-Golay is a type of low-pass filter, particularly\n",
    "#         suited for smoothing noisy data. The main idea behind this\n",
    "#         approach is to make for each point a least-square fit with a\n",
    "#         polynomial of high order over a odd-sized window centered at\n",
    "#         the point.\n",
    "#         Examples\n",
    "#         --------\n",
    "#         t = np.linspace(-4, 4, 500)\n",
    "#         y = np.exp( -t**2 ) + np.random.normal(0, 0.05, t.shape)\n",
    "#         ysg = savitzky_golay(y, window_size=31, order=4)\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.plot(t, y, label='Noisy signal')\n",
    "#         plt.plot(t, np.exp(-t**2), 'k', lw=1.5, label='Original signal')\n",
    "#         plt.plot(t, ysg, 'r', label='Filtered signal')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "#         References\n",
    "#         ----------\n",
    "#         .. [1] A. Savitzky, M. J. E. Golay, Smoothing and Differentiation of\n",
    "#            Data by Simplified Least Squares Procedures. Analytical\n",
    "#            Chemistry, 1964, 36 (8), pp 1627-1639.\n",
    "#         .. [2] Numerical Recipes 3rd Edition: The Art of Scientific Computing\n",
    "#            W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery\n",
    "#            Cambridge University Press ISBN-13: 9780521880688\n",
    "#         \"\"\"\n",
    "#         \n",
    "#         try:\n",
    "#             window_size = np.abs(np.int(window_size))\n",
    "#             order = np.abs(np.int(order))\n",
    "#         except:\n",
    "#             raise ValueError(\"window_size and order have to be of type int\")\n",
    "#         if window_size % 2 != 1 or window_size < 1:\n",
    "#             raise TypeError(\"window_size size must be a positive odd number\")\n",
    "#         if window_size < order + 2:\n",
    "#             raise TypeError(\"window_size is too small for the polynomials order\")\n",
    "#         order_range = range(order+1)\n",
    "#         half_window = (window_size -1) // 2\n",
    "#         # precompute coefficients\n",
    "#         b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])\n",
    "#         m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n",
    "#         # pad the signal at the extremes with\n",
    "#         # values taken from the signal itself\n",
    "#         firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )\n",
    "#         lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])\n",
    "#         y = np.concatenate((firstvals, y, lastvals))\n",
    "#         return np.convolve( m[::-1], y, mode='valid')    \n",
    "# \n",
    "#     def smooth_surfaces(self):\n",
    "#         ''' Use Savitzky-Golay to generate slope and curvature'''\n",
    "#         self.smooth_bare_earth = self.savitzky_golay(self.bare_earth, 9, 1)\n",
    "#         self.slope = self.savitzky_golay(self.smooth_bare_earth, 9, 1, 1)\n",
    "#         self.curvature = self.savitzky_golay(self.slope, 9, 1, 1)\n",
    "#     \n",
    "#     def thresh_data(self, arr, sig, thresh):\n",
    "#         ''' Use Gaussian Filter to create boolean array where True values are\n",
    "#         consecutively greater than some threshold. This is useful where there\n",
    "#         are many drifts arranged in series, e.g. watertracks.\n",
    "#         '''    \n",
    "#         sigma = sig\n",
    "#         threshold = thresh\n",
    "#         self.above_threshold = gaussian_filter(arr, sigma=sigma) > threshold\n",
    "# \n",
    "#     def contiguous_regions(self, condition):\n",
    "#         \"\"\"Finds contiguous True regions of the boolean array \"condition\". Returns\n",
    "#         a 2D array where the first column is the start index of the region and the\n",
    "#         second column is the end index.\"\"\"\n",
    "#     \n",
    "#         # Find the indicies of changes in \"condition\"\n",
    "#         d = np.diff(condition)\n",
    "#         idx, = d.nonzero() \n",
    "#     \n",
    "#         # We need to start things after the change in \"condition\". Therefore, \n",
    "#         # we'll shift the index by 1 to the right.\n",
    "#         idx += 1\n",
    "#     \n",
    "#         if condition[0]:\n",
    "#             # If the start of condition is True prepend a 0\n",
    "#             idx = np.r_[0, idx]\n",
    "#     \n",
    "#         if condition[-1]:\n",
    "#             # If the end of condition is True, append the length of the array\n",
    "#             idx = np.r_[idx, condition.size] # Edit\n",
    "#     \n",
    "#         # Reshape the result into two columns\n",
    "#         idx.shape = (-1,2)\n",
    "#         self.drift_index = idx\n",
    "#     \n",
    "#     def drift_brackets(self):\n",
    "#         ''' Define the start and end of a drift based on some kind of \n",
    "#         topographic indicator. These might need to vary depending on the type\n",
    "#         of terrain'''\n",
    "#         self.min_curve_start = np.nanargmin(self.curvature)\n",
    "#         self.max_curve_end = np.nanargmax(self.curvature)\n",
    "#         \n",
    "#     def measure_flux(self, start, end):\n",
    "#         ''' Use trapezoidal rule integration to calculate flux over the drift\n",
    "#         fetch. Values will be m^3 per lineal m, so essentialy we have m^2 which\n",
    "#         makes sense because we are getting the area under the snow depth curve.\n",
    "#         We divide the initial result by 2 because our data cells are 2m x 2m.\n",
    "#         Parameters are start and end of drift zone for integration.\n",
    "#         '''\n",
    "#         self.flux = integrate.simps(self.snow_depth, range(start, end)) / 2.0\n",
    "#         #self.flux = np.trapz(self.snow_depth[start:end]) / 2.0\n",
    "#         self.drift_length = end - start\n",
    "#         self.avg_slope_under_drift = np.nanmean(self.slope[start:end])\n",
    "# \n",
    "# \n",
    "# ############\n",
    "# def make_transect(bsubsurf, ssubsurf, tline):\n",
    "#     \n",
    "#     tname = Transect(bare_surf.subdict[bsubsurf][tline], \n",
    "#                      snow_depth_surf.subdict[ssubsurf][tline])\n",
    "#     tname.thresh_data(tname.snow_depth,3,np.nanmean(tname.snow_depth))\n",
    "#     tname.smooth_surfaces()\n",
    "#     tname.contiguous_regions(tname.above_threshold == True)\n",
    "#     \n",
    "#     return tname\n",
    "#     \n",
    "#     # uncomment below if entire transect is a drift\n",
    "#     #t1.drift_brackets()\n",
    "#     #t1.measure_flux(t1.min_curve_start,t1.max_curve_end)\n",
    "#     \n",
    "# def make_subdrifts(tname):\n",
    "# \n",
    "#     dct = dict()\n",
    "#     # I expect to see meaningless 'mean of empty slice\n",
    "#     # RuntimeWarnings in this block\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.simplefilter(\"ignore\", category=RuntimeWarning)        \n",
    "#         for d in tname.drift_index:\n",
    "#             dr_id = str(d)[1:-1]\n",
    "#             dct[dr_id] = Subdrift(tname, d)\n",
    "#             dct[dr_id].smooth_surfaces()\n",
    "#             dct[dr_id].measure_flux(dct[dr_id].start,\n",
    "#                       dct[dr_id].end)\n",
    "#     return dct\n",
    "# \n",
    "# # this is cool!\n",
    "# # big flux...low flux...big flux..low flux..then decreasing trend:\n",
    "# def scatter_flux(drift_dct):\n",
    "#     \n",
    "#     fluxes = []\n",
    "#     starts = []\n",
    "#     dlengths = []\n",
    "#     for k,v in drift_dct.items():\n",
    "#         fluxes.append(v.flux)\n",
    "#         starts.append(v.start)\n",
    "#         dlengths.append(v.drift_length)\n",
    "#     #plt.figure()\n",
    "#     plt.scatter(starts,fluxes,s=dlengths)\n",
    "#     plt.ylabel('Drift Flux')\n",
    "#     plt.xlabel('drift start [m]')\n",
    "#     return fluxes\n",
    "# \n",
    "# def plot_overview(im, dct, tline):\n",
    "#     \n",
    "#     dims = np.isnan(im[tline])\n",
    "#     x = np.diff(np.where(dims))\n",
    "#     cutoff = np.where(x>1)\n",
    "#     include_start = cutoff[1][0]\n",
    "#     include_stop = x.max()\n",
    "#     \n",
    "#     plt.figure()\n",
    "#     plt.imshow(im[:,include_start:include_stop])\n",
    "#     \n",
    "#     for k,v in dct.items():\n",
    "#         x1 = v.start / im[:,include_start:include_stop].shape[1]\n",
    "#         x2 = v.end / im[:,include_start:include_stop].shape[1]\n",
    "#         plt.axhline(y=tline, xmin = x1, xmax = x2, c = 'r', alpha=0.5)\n",
    "# \n",
    "# ##############################################################################\n",
    "# tlines = np.arange(220,455,5)\n",
    "# tnames = []\n",
    "# dnames=[]\n",
    "# for y in tlines:\n",
    "#     t = 't5_' + str(y)\n",
    "#     d = 't5_' + str(y) +'_sub'\n",
    "#     tnames.append(t)\n",
    "#     dnames.append(d)\n",
    "#     del y, d, t\n",
    "# \n",
    "# big_t_dct = dict()\n",
    "# for f in zip(tnames ,tlines):\n",
    "#     big_t_dct[f[0]] = make_transect('b5', 's5', f[1])\n",
    "# \n",
    "# big_s_dct = dict()\n",
    "# for f in zip(dnames ,tnames):\n",
    "#     big_s_dct[f[0]] = make_subdrifts(big_t_dct[f[1]])\n",
    "# \n",
    "# all_flux = []\n",
    "# flux_sums = []\n",
    "# for k in big_s_dct.keys():\n",
    "#     f = scatter_flux(big_s_dct[k])\n",
    "#     all_flux.append(f)\n",
    "# \n",
    "# for f in all_flux:\n",
    "#     flux_sums.append(sum(f) / 2)\n",
    "# \n",
    "# plt.scatter(tlines, flux_sums)\n",
    "# \n",
    "# \n",
    "# for k in zip(big_s_dct.keys(), tlines):\n",
    "#     plot_overview(snow_depth_surf.subdict['s5'], big_s_dct[k[0]], k[1])\n",
    "# \n",
    "# ################################################################\n",
    "\n",
    "     2019 GitHub, Inc.\n",
    "    Terms\n",
    "    Privacy\n",
    "    Security\n",
    "    Status\n",
    "    Help\n",
    "\n",
    "    Contact GitHub\n",
    "    Pricing\n",
    "    API\n",
    "    Training\n",
    "    Blog\n",
    "    About\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
